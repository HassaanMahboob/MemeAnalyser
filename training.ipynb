{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\razak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\razak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\razak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Input, Bidirectional, Flatten, Conv2D, MaxPooling2D, concatenate, Conv1D, MaxPooling1D\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from  tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "import tensorflow as tf\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.style as style\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\razak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\razak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\razak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.style as style\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('hateful_memes_expanded/train.jsonl',lines = True , encoding = 'utf-8')\n",
    "\n",
    "df_test_seen = pd.read_json('hateful_memes_expanded/test_seen.jsonl',lines = True , encoding = 'utf-8')\n",
    "df_test_unseen = pd.read_json('hateful_memes_expanded/test_unseen.jsonl',lines = True , encoding = 'utf-8')\n",
    "df_test = df_test_seen.append(df_test_unseen, ignore_index=True)\n",
    "\n",
    "df_dev_seen = pd.read_json('hateful_memes_expanded/dev_seen.jsonl',lines = True , encoding = 'utf-8')\n",
    "df_dev_unseen = pd.read_json('hateful_memes_expanded/dev_unseen.jsonl',lines = True , encoding = 'utf-8')\n",
    "df_val = df_dev_seen.append(df_dev_unseen, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12887\n",
      "3000\n",
      "1040\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(len(df_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12882</th>\n",
       "      <td>fyAh3I0.png</td>\n",
       "      <td>img/fyAh3I0.png</td>\n",
       "      <td>0</td>\n",
       "      <td>WHAT IF POKEMON GO WAS RELEASED TO D US FRO TH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12883</th>\n",
       "      <td>CMQqhImUkAAKeno.jpg</td>\n",
       "      <td>img/CMQqhImUkAAKeno.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Donald Trump 's hair looks like someone tried ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12884</th>\n",
       "      <td>tINjUCc.png</td>\n",
       "      <td>img/tINjUCc.png</td>\n",
       "      <td>0</td>\n",
       "      <td>MAMAS Who am I supposed to vote for ? Am l sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12885</th>\n",
       "      <td>eI2N5iQ.png</td>\n",
       "      <td>img/eI2N5iQ.png</td>\n",
       "      <td>0</td>\n",
       "      <td>EcakpnBeHn rnacoBnl 18 minutes ago We will hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12886</th>\n",
       "      <td>8ZOPW62.png</td>\n",
       "      <td>img/8ZOPW62.png</td>\n",
       "      <td>0</td>\n",
       "      <td>Voting for Trump because Bernie s out is like ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12887 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                      img  label  \\\n",
       "0                    42953            img/42953.png      0   \n",
       "1                    23058            img/23058.png      0   \n",
       "2                    13894            img/13894.png      0   \n",
       "3                    37408            img/37408.png      0   \n",
       "4                    82403            img/82403.png      0   \n",
       "...                    ...                      ...    ...   \n",
       "12882          fyAh3I0.png          img/fyAh3I0.png      0   \n",
       "12883  CMQqhImUkAAKeno.jpg  img/CMQqhImUkAAKeno.jpg      0   \n",
       "12884          tINjUCc.png          img/tINjUCc.png      0   \n",
       "12885          eI2N5iQ.png          img/eI2N5iQ.png      0   \n",
       "12886          8ZOPW62.png          img/8ZOPW62.png      0   \n",
       "\n",
       "                                                    text  \n",
       "0       its their character not their color that matters  \n",
       "1      don't be afraid to love again everyone is not ...  \n",
       "2                               putting bows on your pet  \n",
       "3      i love everything and everybody! except for sq...  \n",
       "4      everybody loves chocolate chip cookies, even h...  \n",
       "...                                                  ...  \n",
       "12882  WHAT IF POKEMON GO WAS RELEASED TO D US FRO TH...  \n",
       "12883  Donald Trump 's hair looks like someone tried ...  \n",
       "12884  MAMAS Who am I supposed to vote for ? Am l sup...  \n",
       "12885  EcakpnBeHn rnacoBnl 18 minutes ago We will hav...  \n",
       "12886  Voting for Trump because Bernie s out is like ...  \n",
       "\n",
       "[12887 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "list_of_pics = list()\n",
    "text = list()\n",
    "label = list()\n",
    "errorfiles = 0 \n",
    "for row in df_train.iterrows():\n",
    "    try:\n",
    "        image=Image.open('hateful_memes_expanded/'+row[1][\"img\"])\n",
    "        image=image.resize((200,200))\n",
    "        list_of_pics.append(np.asarray(image))\n",
    "        text.append(row[1][\"text\"])\n",
    "        label.append(row[1][\"label\"])\n",
    "    except FileNotFoundError:\n",
    "        errorfiles+=1\n",
    "        pass\n",
    "\n",
    "X_img_train = np.array(list_of_pics)\n",
    "X_text_train = np.array(text)\n",
    "Y_train = np.array(label)\n",
    "print(errorfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pics = list()\n",
    "text = list()\n",
    "label = list()\n",
    "for row in df_test.iterrows():\n",
    "    try:\n",
    "        image=Image.open('hateful_memes_expanded/'+row[1][\"img\"])\n",
    "        image=image.resize((200,200))\n",
    "        list_of_pics.append(np.asarray(image))\n",
    "        text.append(row[1][\"text\"])\n",
    "        label.append(row[1][\"label\"])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "X_img_test = np.array(list_of_pics)\n",
    "X_text_test = np.array(text)\n",
    "Y_test = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pics = list()\n",
    "text = list()\n",
    "label = list()\n",
    "for row in df_val.iterrows():\n",
    "    try:\n",
    "        image=Image.open('hateful_memes_expanded/'+row[1][\"img\"])\n",
    "        image=image.resize((200,200))\n",
    "        list_of_pics.append(np.asarray(image))\n",
    "        text.append(row[1][\"text\"])\n",
    "        label.append(row[1][\"label\"])\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "X_img_val = np.array(list_of_pics)\n",
    "X_text_val = np.array(text)\n",
    "Y_val = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12887\n",
      "12887\n",
      "12887\n"
     ]
    }
   ],
   "source": [
    "print(len(X_img_train))\n",
    "print(len(X_text_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "\n",
    "dataset[\"X_img_val\"] = X_img_val\n",
    "dataset[\"X_text_val\"] = X_text_val\n",
    "dataset[\"Y_val\"] = Y_val\n",
    "\n",
    "dataset[\"X_img_test\"] = X_img_test\n",
    "dataset[\"X_text_test\"] = X_text_test\n",
    "dataset[\"Y_test\"] = Y_test\n",
    "\n",
    "dataset[\"X_img_train\"] = X_img_train\n",
    "dataset[\"X_text_train\"] = X_text_train\n",
    "dataset[\"Y_train\"] = Y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(dataset, open(\"dataset.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset = pickle.load(open(\"dataset.pk\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img_val = dataset[\"X_img_val\"]\n",
    "X_text_val = dataset[\"X_text_val\"]\n",
    "Y_val = dataset[\"Y_val\"] \n",
    "\n",
    "X_img_test = dataset[\"X_img_test\"]\n",
    "X_text_test = dataset[\"X_text_test\"]\n",
    "Y_test = dataset[\"Y_test\"]\n",
    "\n",
    "X_img_train = dataset[\"X_img_train\"]\n",
    "X_text_train = dataset[\"X_text_train\"]\n",
    "Y_train = dataset[\"Y_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Input, Bidirectional, Flatten, Conv2D, MaxPooling2D, concatenate, Conv1D, MaxPooling1D\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from  tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20241 unique tokens.\n",
      "Max len: 1601\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = pd.Series(X_text_train).map(len).max()\n",
    "EMBEDDING_DIM = 300\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(X_text_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print('Max len:', MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\razak\\AppData\\Local\\Temp/ipykernel_8260/2712980230.py:5: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embeddings_index[word] = word2vecDict.word_vec(word)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000000 word vectors.\n",
      "total embedded: 12929 common words\n"
     ]
    }
   ],
   "source": [
    "word2vecDict = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "embed_size = 300\n",
    "embeddings_index = dict()\n",
    "for word in word2vecDict.key_to_index:\n",
    "  embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
    "      \n",
    "embedding_matrix = 1 * np.random.randn(len(word_index)+1, embed_size)\n",
    "\n",
    "embeddedCount = 0\n",
    "for word, i in word_index.items():\n",
    "  i-=1\n",
    "  embedding_vector = embeddings_index.get(word)\n",
    "  if embedding_vector is not None: \n",
    "    embedding_matrix[i] = embedding_vector\n",
    "    embeddedCount+=1\n",
    "print(\"total embedded:\", embeddedCount, \"common words\")\n",
    "      \n",
    "del(embeddings_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train tensor: (12887, 1601)\n"
     ]
    }
   ],
   "source": [
    "X_text_train = tokenizer.texts_to_sequences(X_text_train)\n",
    "X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train tensor:', X_text_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train tensor: (3000, 1601)\n"
     ]
    }
   ],
   "source": [
    "X_text_test = tokenizer.texts_to_sequences(X_text_test)\n",
    "X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train tensor:', X_text_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train tensor: (1040, 1601)\n"
     ]
    }
   ],
   "source": [
    "X_text_val = tokenizer.texts_to_sequences(X_text_val)\n",
    "X_text_val = pad_sequences(X_text_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train tensor:', X_text_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200 3\n"
     ]
    }
   ],
   "source": [
    "lstm_input = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], mask_zero=True, input_length=MAX_SEQUENCE_LENGTH, trainable=False)(lstm_input)\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(64, return_sequences = True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(64)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "lstm_out = Dense(18, activation = 'relu')(x)\n",
    "\n",
    "print(X_img_train.shape[1], X_img_train.shape[2], X_img_train.shape[3])\n",
    "cnn_input = Input(shape=(X_img_train.shape[1], X_img_train.shape[2], X_img_train.shape[3]))\n",
    "y = Conv2D(32, (3, 3), activation='relu', input_shape=(X_img_train.shape[1], X_img_train.shape[2], X_img_train.shape[3]))(cnn_input)\n",
    "y = MaxPooling2D(2, 2)(y)\n",
    "y = Conv2D(64, (3, 3), activation='relu')(y)\n",
    "y = MaxPooling2D(2, 2)(y)\n",
    "y = Conv2D(128, (3, 3), activation='relu')(y)\n",
    "y = MaxPooling2D(2, 2)(y)\n",
    "y = Conv2D(128, (3, 3), activation='relu')(y)\n",
    "y = MaxPooling2D(2, 2)(y)\n",
    "y = Flatten()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "cnn_out = Dense(512, activation='relu')(y)\n",
    "\n",
    "concat_inp = concatenate([cnn_out, lstm_out])\n",
    "z = Dense(256, activation='relu')(concat_inp)\n",
    "z = Dropout(0.3)(z)\n",
    "z = Dense(128, activation='relu')(z)\n",
    "z = Dropout(0.3)(z)\n",
    "output = Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "model = Model(inputs=[cnn_input, lstm_input], outputs=[output])\n",
    "adam = Adam(learning_rate=0.001, decay=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 198, 198, 32) 896         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 99, 99, 32)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 97, 97, 64)   18496       max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 48, 48, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 1601)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 46, 46, 128)  73856       max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1601, 300)    6072600     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 23, 23, 128)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 1601, 300)    0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 21, 21, 128)  147584      max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 1601, 64)     93440       dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 10, 10, 128)  0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 1601, 64)     0           lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 12800)        0           max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 64)           33024       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 12800)        0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 64)           0           lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 512)          6554112     dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 18)           1170        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 530)          0           dense_14[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 256)          135936      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 256)          0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 128)          32896       dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 128)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1)            129         dropout_23[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,164,139\n",
      "Trainable params: 7,091,539\n",
      "Non-trainable params: 6,072,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=5)\n",
    "csv_logger = CSVLogger('model_history_log_model_lstm.csv', append=True)\n",
    "check_point = ModelCheckpoint(filepath='best_lstm_model_todate', save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "51/51 [==============================] - 11494s 223s/step - loss: 2.8514 - accuracy: 0.5269 - val_loss: 0.7034 - val_accuracy: 0.4394\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "Failed to create a directory: /; Permission denied [Op:MergeV2Checkpoints]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8260/817631884.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m multimodal_model = model.fit([X_img_train, X_text_train], Y_train, batch_size=256, epochs=20,\n\u001b[0m\u001b[0;32m      2\u001b[0m                             validation_data=([X_img_val, X_text_val], Y_val), callbacks = [es, csv_logger, check_point], verbose=1)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'epoch'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1416\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m                 self.model.save_weights(\n\u001b[0m\u001b[0;32m   1419\u001b[0m                     filepath, overwrite=True, options=self._options)\n\u001b[0;32m   1420\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[0;32m   2222\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2223\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2225\u001b[0m       \u001b[1;31m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m       checkpoint_management.update_checkpoint_state_internal(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session, options)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[0m\u001b[0;32m   1219\u001b[0m         file_prefix_tensor, object_graph_tensor, options)\n\u001b[0;32m   1220\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36m_save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor, options)\u001b[0m\n\u001b[0;32m   1162\u001b[0m         or context.executing_eagerly() or ops.inside_function()):\n\u001b[0;32m   1163\u001b[0m       \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDeviceSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_saveable_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1164\u001b[1;33m       \u001b[0msave_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1165\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[0mtf_function_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msave_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m           \u001b[1;31m# merged, attempts to delete the temporary directory,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m           \u001b[1;31m# \"<user-fed prefix>_temp\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m           return gen_io_ops.merge_v2_checkpoints(\n\u001b[0m\u001b[0;32m    287\u001b[0m               sharded_prefixes, file_prefix, delete_old_dirs=True)\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mmerge_v2_checkpoints\u001b[1;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name)\u001b[0m\n\u001b[0;32m    499\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m       return merge_v2_checkpoints_eager_fallback(\n\u001b[0m\u001b[0;32m    502\u001b[0m           \u001b[0mcheckpoint_prefixes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m           delete_old_dirs=delete_old_dirs, name=name, ctx=_ctx)\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\u001b[0m in \u001b[0;36mmerge_v2_checkpoints_eager_fallback\u001b[1;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name, ctx)\u001b[0m\n\u001b[0;32m    524\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_prefixes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_prefix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"delete_old_dirs\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelete_old_dirs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m   _result = _execute.execute(b\"MergeV2Checkpoints\", 0, inputs=_inputs_flat,\n\u001b[0m\u001b[0;32m    527\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0;32m    528\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionDeniedError\u001b[0m: Failed to create a directory: /; Permission denied [Op:MergeV2Checkpoints]"
     ]
    }
   ],
   "source": [
    "multimodal_model = model.fit([X_img_train, X_text_train], Y_train, batch_size=256, epochs=20,\n",
    "                            validation_data=([X_img_val, X_text_val], Y_val), callbacks = [es, csv_logger, check_point], verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5cf37a3177b5584b60c2291b6138bd1a3d691b566dd956db489f32b7f6d050e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
